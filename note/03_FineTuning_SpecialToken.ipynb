{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eaed98d",
   "metadata": {},
   "source": [
    "# 질문자\n",
    "```\n",
    "잘된다. 노무현으로만 했는데 모든 대통령을 각각 훈련시켜서 인자로 노무현, 국민 여러분, 우리나라의 미래는 또는 박정희, 국민 여러분, 우리나라의 미래는 이런 식으로 가능할까?\n",
    "```\n",
    "\n",
    "# Gemini\n",
    "```\n",
    "네, 물론 가능합니다! 그리고 그것이 바로 파인튜닝의 진정한 묘미 중 하나입니다. 한 명의 스타일에만 특화된 모델을 넘어, 하나의 모델이 여러 인물의 스타일을 학습하고, 필요에 따라 특정 인물의 스타일을 흉내 내도록 만들 수 있습니다.\n",
    "\n",
    "사용자께서 원하시는 기능, 즉 (대통령, 시작 문장)을 인자로 받아 해당 대통령의 스타일로 뒷문장을 생성하는 것은 매우 효과적인 파인튜닝 기법으로 구현할 수 있습니다.\n",
    "\n",
    "가장 전문적이고 널리 쓰이는 방법은 '스페셜 토큰(Special Token)'을 활용하는 것입니다.\n",
    "\n",
    "'스페셜 토큰'을 이용한 멀티 스타일 모델 구현 전략\n",
    "핵심 아이디어는 간단합니다. 모델에게 어떤 대통령의 스타일로 글을 써야 할지 알려주는 '꼬리표'를 붙여서 학습시키는 것입니다.\n",
    "\n",
    "스페셜 토큰 정의: 각 대통령의 이름을 나타내는 특별한 토큰을 만듭니다. (예: <|노무현|>, <|박정희|>)\n",
    "토크나이저 확장: 이 스페셜 토큰들을 모델이 이해할 수 있는 새로운 단어로 토크나이저에 정식 등록합니다.\n",
    "데이터 전처리: 모든 학습 데이터의 맨 앞에 해당 연설을 한 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "학습 데이터 예시: <|노무현|> 국민 여러분, 우리나라의 미래는 우리 스스로가 만들어가야 합니다.\n",
    "모델 학습: 이렇게 가공된 데이터로 모델을 파인튜닝합니다. 모델은 스페셜 토큰과 그 뒤에 이어지는 텍스트 스타일 간의 연관성을 학습하게 됩니다.\n",
    "조건부 생성: 텍스트를 생성할 때, 우리가 원하는 대통령의 스페셜 토큰으로 문장을 시작하면, 모델이 해당 스타일을 따라 글을 생성하게 됩니다.\n",
    "생성 요청 예시: <|박정희|> 국민 여러분, 우리나라의 미래는\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f826f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1c85a4-e969-4f86-bdc8-84ac3f956cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475b9c1",
   "metadata": {},
   "source": [
    "# --- 1. 데이터 로딩 및 스페셜 토큰 정의 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09eb719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 연설문 개수: 8771\n"
     ]
    }
   ],
   "source": [
    "from president_speech.db.parquet_interpreter import read_parquet\n",
    "df = read_parquet()\n",
    "# 여기서는 모든 대통령 데이터를 사용하므로 '노무현'으로 필터링하지 않습니다.\n",
    "print(f\"전체 연설문 개수: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8841d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습할 대통령 목록: ['박정희', '최규하', '전두환', '노태우', '김영삼', '김대중', '노무현', '이명박', '이승만', '윤보선', '박근혜', '문재인']\n"
     ]
    }
   ],
   "source": [
    "# 1-1. 데이터에 있는 모든 대통령의 목록을 가져옵니다.\n",
    "presidents = df['president'].unique().tolist()\n",
    "print(f\"학습할 대통령 목록: {presidents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6598448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 스페셜 토큰: ['<|박정희|>', '<|최규하|>', '<|전두환|>', '<|노태우|>', '<|김영삼|>', '<|김대중|>', '<|노무현|>', '<|이명박|>', '<|이승만|>', '<|윤보선|>', '<|박근혜|>', '<|문재인|>']\n"
     ]
    }
   ],
   "source": [
    "# 1-2. 각 대통령을 위한 스페셜 토큰을 정의합니다.\n",
    "special_tokens = [f\"<|{p}|>\" for p in presidents]\n",
    "print(f\"생성된 스페셜 토큰: {special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6ac0a",
   "metadata": {},
   "source": [
    "# --- 2. 토크나이저에 스페셜 토큰 추가 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03f851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'skt/kogpt2-base-v2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeafa042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-1. 정의한 스페셜 토큰들을 토크나이저의 어휘(vocabulary)에 추가합니다.\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ecefb",
   "metadata": {},
   "source": [
    "# --- 3. 모델의 토큰 임베딩 리사이즈 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8691df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51212, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1. 중요: 토크나이저에 새로운 단어(토큰)가 추가되었으므로,\n",
    "# 모델도 이 새로운 단어를 이해할 수 있도록 '뇌의 용량(임베딩 레이어)'을 늘려줘야 합니다.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8009ff",
   "metadata": {},
   "source": [
    "# --- 4. 데이터셋 구성 방식 변경 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d684ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1. 각 연설문 텍스트 앞에 해당 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "formatted_texts = []\n",
    "for index, row in df.iterrows():\n",
    "    president_token = f\"<|{row['president']}|>\"\n",
    "    # 예: \"<|노무현|> 국민 여러분, ...\"\n",
    "    formatted_text = f\"{president_token} {row['speech_text']}\"\n",
    "    formatted_texts.append(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e8b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2. 스페셜 토큰이 추가된 모든 텍스트를 하나로 결합합니다.\n",
    "combined_text = \"\\n\\n\".join(formatted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "365bf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3. Dataset 객체 생성 및 토크나이징, 청킹 (이하 과정은 이전과 동일)\n",
    "raw_dataset = Dataset.from_dict({'text': [combined_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e4a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_and_chunk 함수는 이전 코드와 동일하게 사용합니다.\n",
    "# ... (def tokenize_and_chunk ... 부분)\n",
    "# 3-2. 데이터셋 토크나이징 및 청킹(Chunking)\n",
    "# 텍스트를 토큰 ID로 변환하고, 긴 텍스트를 일정한 길이(block_size)의 덩어리로 자릅니다.\n",
    "def tokenize_and_chunk(examples):\n",
    "    # 전체 텍스트를 토크나이징합니다.\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=False) # 긴 텍스트이므로 자르지 않음\n",
    "\n",
    "    block_size = 128  # 한 번에 처리할 토큰의 수 (GPU 메모리에 따라 조절)\n",
    "    \n",
    "    # 결과를 저장할 딕셔너리\n",
    "    result = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    # 모든 토큰을 하나로 합칩니다.\n",
    "    concatenated_ids = sum(tokenized_output['input_ids'], [])\n",
    "    total_length = len(concatenated_ids)\n",
    "\n",
    "    # block_size 단위로 텍스트를 나눕니다.\n",
    "    for i in range(0, total_length, block_size):\n",
    "        chunk = concatenated_ids[i:i + block_size]\n",
    "        \n",
    "        # 마지막 청크가 block_size보다 작으면 패딩을 추가합니다.\n",
    "        if len(chunk) < block_size:\n",
    "            padding_length = block_size - len(chunk)\n",
    "            chunk.extend([tokenizer.pad_token_id] * padding_length)\n",
    "            \n",
    "        result[\"input_ids\"].append(chunk)\n",
    "        # 패딩 토큰은 attention 계산에서 제외하기 위해 0으로 설정\n",
    "        attention_mask = [1] * (len(chunk) - chunk.count(tokenizer.pad_token_id)) + [0] * chunk.count(tokenizer.pad_token_id)\n",
    "        result[\"attention_mask\"].append(attention_mask)\n",
    "        # Causal LM 파인튜닝에서는 input_ids와 labels를 동일하게 설정합니다.\n",
    "        # Trainer가 내부적으로 labels를 한 칸씩 밀어서(shift) 처리해줍니다.\n",
    "        result[\"labels\"].append(chunk)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ab6a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9659c603f448c2b71c7d7288c99fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멀티 스타일 파인튜닝을 위한 데이터셋 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = raw_dataset.map(tokenize_and_chunk, batched=True, remove_columns=raw_dataset.column_names)\n",
    "print(\"멀티 스타일 파인튜닝을 위한 데이터셋 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc696ca2",
   "metadata": {},
   "source": [
    "# --- 5. 학습 및 저장 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ccb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments, DataCollator, Trainer 초기화 및 학습 과정은 이전과 완벽하게 동일합니다.\n",
    "# output_dir 이름만 변경하여 이전 모델과 겹치지 않게 합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./all_presidents_gpt2_results', # 결과 저장 디렉터리 변경\n",
    "    # ... (num_train_epochs, batch_size 등 나머지 설정은 동일)\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41923929",
   "metadata": {},
   "source": [
    "- 정상 결과: Trainer is configured to use device: cuda:0 와 같이 cuda가 포함된 장치 이름이 출력되어야 합니다.\n",
    "- 문제 상황: 만약 Trainer is configured to use device: cpu 라고 출력된다면, Trainer가 GPU를 사용하지 않기로 결정했다는 의미입니다.\n",
    "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7438483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ 이 코드를 추가하여 확인 ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼\n",
    "print(f\"Trainer is configured to use device: {trainer.args.device}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8627' max='49281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8627/49281 14:18 < 1:07:24, 10.05 it/s, Epoch 0.53/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.918600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.806200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>3.807700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.784200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.748900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.787200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.733600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 저장\n",
    "final_model_path = './all_presidents_gpt2_final'\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"멀티 스타일 모델이 '{final_model_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 조건부 텍스트 생성 테스트 ---\n",
    "# 저장된 멀티 스타일 모델과 토크나이저를 다시 불러옵니다.\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(final_model_path)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft.to(device)\n",
    "\n",
    "# 생성 함수를 약간 수정하여 대통령 이름을 인자로 받도록 합니다.\n",
    "def generate_text_by_president(president_name, prompt, max_len=128):\n",
    "    # 입력 프롬프트 앞에 해당 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "    president_token = f\"<|{president_name}|>\"\n",
    "    formatted_prompt = f\"{president_token} {prompt}\"\n",
    "    \n",
    "    print(f\"입력: {formatted_prompt}\")\n",
    "\n",
    "    input_ids = tokenizer_ft.encode(formatted_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    gen_ids = model_ft.generate(input_ids,\n",
    "                               max_length=max_len,\n",
    "                               repetition_penalty=2.0,\n",
    "                               pad_token_id=tokenizer_ft.pad_token_id,\n",
    "                               eos_token_id=tokenizer_ft.eos_token_id,\n",
    "                               bos_token_id=tokenizer_ft.bos_token_id,\n",
    "                               use_cache=True,\n",
    "                               do_sample=True, # 더 자연스러운 문장을 위해 샘플링 사용\n",
    "                               top_k=50,\n",
    "                               top_p=0.95)\n",
    "    \n",
    "    generated_text = tokenizer_ft.decode(gen_ids[0], skip_special_tokens=False) # 스페셜 토큰도 보이게 출력\n",
    "    # 스페셜 토큰을 제외하고 싶다면 skip_special_tokens=True 로 변경\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6928724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "print(\"\\n--- 멀티 스타일 모델 생성 테스트 ---\")\n",
    "prompt = \"국민 여러분, 우리나라의 미래는\"\n",
    "\n",
    "# '노무현' 스타일로 생성\n",
    "print(f\"생성 결과 (노무현): {generate_text_by_president('노무현', prompt)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# '박정희' 스타일로 생성 (데이터에 '박정희'가 있다고 가정)\n",
    "print(f\"생성 결과 (박정희): {generate_text_by_president('박정희', prompt)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# '김대중' 스타일로 생성 (데이터에 '김대중'이 있다고 가정)\n",
    "print(f\"생성 결과 (김대중): {generate_text_by_president('김대중', prompt)}\")\n",
    "print(\"--- 테스트 완료 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
