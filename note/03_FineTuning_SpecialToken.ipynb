{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eaed98d",
   "metadata": {},
   "source": [
    "# 질문자\n",
    "```\n",
    "잘된다. 노무현으로만 했는데 모든 대통령을 각각 훈련시켜서 인자로 노무현, 국민 여러분, 우리나라의 미래는 또는 박정희, 국민 여러분, 우리나라의 미래는 이런 식으로 가능할까?\n",
    "```\n",
    "\n",
    "# Gemini\n",
    "```\n",
    "네, 물론 가능합니다! 그리고 그것이 바로 파인튜닝의 진정한 묘미 중 하나입니다. 한 명의 스타일에만 특화된 모델을 넘어, 하나의 모델이 여러 인물의 스타일을 학습하고, 필요에 따라 특정 인물의 스타일을 흉내 내도록 만들 수 있습니다.\n",
    "\n",
    "사용자께서 원하시는 기능, 즉 (대통령, 시작 문장)을 인자로 받아 해당 대통령의 스타일로 뒷문장을 생성하는 것은 매우 효과적인 파인튜닝 기법으로 구현할 수 있습니다.\n",
    "\n",
    "가장 전문적이고 널리 쓰이는 방법은 '스페셜 토큰(Special Token)'을 활용하는 것입니다.\n",
    "\n",
    "'스페셜 토큰'을 이용한 멀티 스타일 모델 구현 전략\n",
    "핵심 아이디어는 간단합니다. 모델에게 어떤 대통령의 스타일로 글을 써야 할지 알려주는 '꼬리표'를 붙여서 학습시키는 것입니다.\n",
    "\n",
    "스페셜 토큰 정의: 각 대통령의 이름을 나타내는 특별한 토큰을 만듭니다. (예: <|노무현|>, <|박정희|>)\n",
    "토크나이저 확장: 이 스페셜 토큰들을 모델이 이해할 수 있는 새로운 단어로 토크나이저에 정식 등록합니다.\n",
    "데이터 전처리: 모든 학습 데이터의 맨 앞에 해당 연설을 한 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "학습 데이터 예시: <|노무현|> 국민 여러분, 우리나라의 미래는 우리 스스로가 만들어가야 합니다.\n",
    "모델 학습: 이렇게 가공된 데이터로 모델을 파인튜닝합니다. 모델은 스페셜 토큰과 그 뒤에 이어지는 텍스트 스타일 간의 연관성을 학습하게 됩니다.\n",
    "조건부 생성: 텍스트를 생성할 때, 우리가 원하는 대통령의 스페셜 토큰으로 문장을 시작하면, 모델이 해당 스타일을 따라 글을 생성하게 됩니다.\n",
    "생성 요청 예시: <|박정희|> 국민 여러분, 우리나라의 미래는\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f826f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1c85a4-e969-4f86-bdc8-84ac3f956cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475b9c1",
   "metadata": {},
   "source": [
    "# --- 1. 데이터 로딩 및 스페셜 토큰 정의 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09eb719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 연설문 개수: 8771\n"
     ]
    }
   ],
   "source": [
    "from president_speech.db.parquet_interpreter import read_parquet\n",
    "df = read_parquet()\n",
    "# 여기서는 모든 대통령 데이터를 사용하므로 '노무현'으로 필터링하지 않습니다.\n",
    "print(f\"전체 연설문 개수: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8841d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습할 대통령 목록: ['박정희', '최규하', '전두환', '노태우', '김영삼', '김대중', '노무현', '이명박', '이승만', '윤보선', '박근혜', '문재인']\n"
     ]
    }
   ],
   "source": [
    "# 1-1. 데이터에 있는 모든 대통령의 목록을 가져옵니다.\n",
    "presidents = df['president'].unique().tolist()\n",
    "print(f\"학습할 대통령 목록: {presidents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6598448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 스페셜 토큰: ['<|박정희|>', '<|최규하|>', '<|전두환|>', '<|노태우|>', '<|김영삼|>', '<|김대중|>', '<|노무현|>', '<|이명박|>', '<|이승만|>', '<|윤보선|>', '<|박근혜|>', '<|문재인|>']\n"
     ]
    }
   ],
   "source": [
    "# 1-2. 각 대통령을 위한 스페셜 토큰을 정의합니다.\n",
    "special_tokens = [f\"<|{p}|>\" for p in presidents]\n",
    "print(f\"생성된 스페셜 토큰: {special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6ac0a",
   "metadata": {},
   "source": [
    "# --- 2. 토크나이저에 스페셜 토큰 추가 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03f851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'skt/kogpt2-base-v2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeafa042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-1. 정의한 스페셜 토큰들을 토크나이저의 어휘(vocabulary)에 추가합니다.\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ecefb",
   "metadata": {},
   "source": [
    "# --- 3. 모델의 토큰 임베딩 리사이즈 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8691df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51212, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1. 중요: 토크나이저에 새로운 단어(토큰)가 추가되었으므로,\n",
    "# 모델도 이 새로운 단어를 이해할 수 있도록 '뇌의 용량(임베딩 레이어)'을 늘려줘야 합니다.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8009ff",
   "metadata": {},
   "source": [
    "# --- 4. 데이터셋 구성 방식 변경 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d684ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1. 각 연설문 텍스트 앞에 해당 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "formatted_texts = []\n",
    "for index, row in df.iterrows():\n",
    "    president_token = f\"<|{row['president']}|>\"\n",
    "    # 예: \"<|노무현|> 국민 여러분, ...\"\n",
    "    formatted_text = f\"{president_token} {row['speech_text']}\"\n",
    "    formatted_texts.append(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4e8b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2. 스페셜 토큰이 추가된 모든 텍스트를 하나로 결합합니다.\n",
    "combined_text = \"\\n\\n\".join(formatted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365bf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3. Dataset 객체 생성 및 토크나이징, 청킹 (이하 과정은 이전과 동일)\n",
    "raw_dataset = Dataset.from_dict({'text': [combined_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e4a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_and_chunk 함수는 이전 코드와 동일하게 사용합니다.\n",
    "# ... (def tokenize_and_chunk ... 부분)\n",
    "# 3-2. 데이터셋 토크나이징 및 청킹(Chunking)\n",
    "# 텍스트를 토큰 ID로 변환하고, 긴 텍스트를 일정한 길이(block_size)의 덩어리로 자릅니다.\n",
    "def tokenize_and_chunk(examples):\n",
    "    # 전체 텍스트를 토크나이징합니다.\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=False) # 긴 텍스트이므로 자르지 않음\n",
    "\n",
    "    block_size = 128  # 한 번에 처리할 토큰의 수 (GPU 메모리에 따라 조절)\n",
    "    \n",
    "    # 결과를 저장할 딕셔너리\n",
    "    result = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    # 모든 토큰을 하나로 합칩니다.\n",
    "    concatenated_ids = sum(tokenized_output['input_ids'], [])\n",
    "    total_length = len(concatenated_ids)\n",
    "\n",
    "    # block_size 단위로 텍스트를 나눕니다.\n",
    "    for i in range(0, total_length, block_size):\n",
    "        chunk = concatenated_ids[i:i + block_size]\n",
    "        \n",
    "        # 마지막 청크가 block_size보다 작으면 패딩을 추가합니다.\n",
    "        if len(chunk) < block_size:\n",
    "            padding_length = block_size - len(chunk)\n",
    "            chunk.extend([tokenizer.pad_token_id] * padding_length)\n",
    "            \n",
    "        result[\"input_ids\"].append(chunk)\n",
    "        # 패딩 토큰은 attention 계산에서 제외하기 위해 0으로 설정\n",
    "        attention_mask = [1] * (len(chunk) - chunk.count(tokenizer.pad_token_id)) + [0] * chunk.count(tokenizer.pad_token_id)\n",
    "        result[\"attention_mask\"].append(attention_mask)\n",
    "        # Causal LM 파인튜닝에서는 input_ids와 labels를 동일하게 설정합니다.\n",
    "        # Trainer가 내부적으로 labels를 한 칸씩 밀어서(shift) 처리해줍니다.\n",
    "        result[\"labels\"].append(chunk)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33ab6a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f3a30eff6041e8b470ba1475457955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멀티 스타일 파인튜닝을 위한 데이터셋 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = raw_dataset.map(tokenize_and_chunk, batched=True, remove_columns=raw_dataset.column_names)\n",
    "print(\"멀티 스타일 파인튜닝을 위한 데이터셋 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc696ca2",
   "metadata": {},
   "source": [
    "# --- 5. 학습 및 저장 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ccb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments, DataCollator, Trainer 초기화 및 학습 과정은 이전과 완벽하게 동일합니다.\n",
    "# output_dir 이름만 변경하여 이전 모델과 겹치지 않게 합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./all_presidents_gpt2_results', # 결과 저장 디렉터리 변경\n",
    "    # ... (num_train_epochs, batch_size 등 나머지 설정은 동일)\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41923929",
   "metadata": {},
   "source": [
    "- 정상 결과: Trainer is configured to use device: cuda:0 와 같이 cuda가 포함된 장치 이름이 출력되어야 합니다.\n",
    "- 문제 상황: 만약 Trainer is configured to use device: cpu 라고 출력된다면, Trainer가 GPU를 사용하지 않기로 결정했다는 의미입니다.\n",
    "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7438483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer is configured to use device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ 이 코드를 추가하여 확인 ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼\n",
    "print(f\"Trainer is configured to use device: {trainer.args.device}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab2cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12321' max='12321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12321/12321 44:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.588900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.559500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>3.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>3.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>3.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>3.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>3.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>3.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>3.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>3.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>3.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>3.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>3.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>3.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>3.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>3.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>3.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>3.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>3.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>3.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>3.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>3.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>3.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>3.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>3.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>3.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>3.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>3.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>3.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>3.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>3.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>3.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>3.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>3.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>3.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>3.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>3.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>3.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>3.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>3.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>3.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>3.057300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12321, training_loss=3.322305152520916, metrics={'train_runtime': 2670.1897, 'train_samples_per_second': 73.822, 'train_steps_per_second': 4.614, 'total_flos': 1.2876340690944e+16, 'train_loss': 3.322305152520916, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19f6f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멀티 스타일 모델이 './all_presidents_gpt2_final'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 최종 모델 저장\n",
    "final_model_path = './all_presidents_gpt2_final'\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"멀티 스타일 모델이 '{final_model_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f206d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 조건부 텍스트 생성 테스트 ---\n",
    "# 저장된 멀티 스타일 모델과 토크나이저를 다시 불러옵니다.\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(final_model_path)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft.to(device)\n",
    "\n",
    "# 생성 함수를 약간 수정하여 대통령 이름을 인자로 받도록 합니다.\n",
    "def generate_text_by_president(president_name, prompt, max_len=128):\n",
    "    # 입력 프롬프트 앞에 해당 대통령의 스페셜 토큰을 붙여줍니다.\n",
    "    president_token = f\"<|{president_name}|>\"\n",
    "    formatted_prompt = f\"{president_token} {prompt}\"\n",
    "    \n",
    "    print(f\"입력: {formatted_prompt}\")\n",
    "\n",
    "    input_ids = tokenizer_ft.encode(formatted_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    gen_ids = model_ft.generate(input_ids,\n",
    "                               max_length=max_len,\n",
    "                               repetition_penalty=2.0,\n",
    "                               pad_token_id=tokenizer_ft.pad_token_id,\n",
    "                               eos_token_id=tokenizer_ft.eos_token_id,\n",
    "                               bos_token_id=tokenizer_ft.bos_token_id,\n",
    "                               use_cache=True,\n",
    "                               do_sample=True, # 더 자연스러운 문장을 위해 샘플링 사용\n",
    "                               top_k=50,\n",
    "                               top_p=0.95)\n",
    "    \n",
    "    generated_text = tokenizer_ft.decode(gen_ids[0], skip_special_tokens=False) # 스페셜 토큰도 보이게 출력\n",
    "    # 스페셜 토큰을 제외하고 싶다면 skip_special_tokens=True 로 변경\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6928724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 멀티 스타일 모델 생성 테스트 ---\n",
      "입력: <|노무현|> 북한은\n",
      "생성 결과 (노무현): <|노무현|> 북한은 그동안 미국과의 관계 정상화를 위한 각종 교류와 지원을 아끼지 않고 있으며 특히 우리의 대북 경수로 지원사업과 한반도 평화구축 사업에 대한 적극적 지지를 표명하고 있습니다. 나는 미, 북 양측이 이러한 모든 진전을 바탕으로 보다 포괄적인 차원의 대화 노력을 기울여 올 것이라고 확신하는 바입니다. 기자 올림픽을 통해 드러난 한국의 역동성과 함께 미국의 아시아 중시 정책과 한, 중 관계는 어떻게 달라지셨는지요. 대통령 저는 지난 5년여 동안 아시아, 태평양지역 국가들과의 협력관계를 크게 진전시켜 온 것을 높이 평가하며, 앞으로도 계속 노력할 것임을 약속드립니다. 미국과 중국, 일본은 아시아의 미래를 책임진 친구들이라는 사실을 잘 알고 있는데 이번 베이징올림픽에서\n",
      "------------------------------\n",
      "입력: <|박정희|> 북한은\n",
      "생성 결과 (박정희): <|박정희|> 북한은 지금도 우리 정부를 전복시키려는 전쟁 도발을 조금도 하지 않고 있습니다. 그들은 적화 통일을 기도하며 온갖 책동과 수단과 방법을 가리지 않는 것입니다. 지난 1년 사이에도 휴전선을 사이에 둔 남침용 땅굴을 파고 있는 것은 바로 그들의 반민족적 흉계임을 금할 수가 없습니다. 이제 북한 침략주의자들은 우리의 남북 관계 개선 노력을 외면하는 한 언제 어디서 어떠한 형태의 무력도발이나 폭력 만행을 할 것으로 예측됩니다. 이러한 일이 일어난다면, 우리는 그 누구도 넘볼 수는 없는 것이며, 만일 북한의 이같은 무모한 군사적 모험이 성공한다면 그들이 가지고 있을 여러 가지 후방 교란 공작 등 모든 수단을 동원해 단호히 응징해야 하겠\n",
      "------------------------------\n",
      "입력: <|김대중|> 북한은\n",
      "생성 결과 (김대중): <|김대중|> 북한은 금년에 들어와서 유엔, 미국, 유럽연합 등 3대 국제기구에 가입하는 나라들로 들어가고 있습니다. 이번에 북한과의 대화 재개를 통한 한반도 평화 정착을 위한 획기적인 조처를 취하고 있는 것은 대단히 바람직한 일이라고 생각합니다. 그러나 북한이 우리를 해치려는 어떤 의도도 가지고 있지 않다는 것을 보여줘야 합니다. 앞으로 북한의 태도라든가 그 변화에 따라 남북한의 화해협력도, 민족화합에 대한 기대심도 한층 높아지고 있다고 봅니다. 다만 일부에서는 아직도 북한에 대해 적대적인 발언과 시혜적 대결을 원하지는 않는다는 이야기도 나오고 있고요. 이러한 상황에서 북이 내부적으로 어려움이 많습니다. 그동안에는 상당한 지도자의 의견을 수렴한 다음에 상당히 긍정적인 평가를\n",
      "--- 테스트 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "print(\"\\n--- 멀티 스타일 모델 생성 테스트 ---\")\n",
    "prompt = \"북한은\"\n",
    "\n",
    "# '노무현' 스타일로 생성\n",
    "print(f\"생성 결과 (노무현): {generate_text_by_president('노무현', prompt)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# '박정희' 스타일로 생성 (데이터에 '박정희'가 있다고 가정)\n",
    "print(f\"생성 결과 (박정희): {generate_text_by_president('박정희', prompt)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# '김대중' 스타일로 생성 (데이터에 '김대중'이 있다고 가정)\n",
    "print(f\"생성 결과 (김대중): {generate_text_by_president('김대중', prompt)}\")\n",
    "print(\"--- 테스트 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecb054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
